1
00:00:00,000 --> 00:00:05,000
In the previous lecture we had a very quick introduction about Grafana, Loki and promptaile.

2
00:00:05,000 --> 00:00:11,000
You may have many questions how to implement these log aggregation without making any changes inside

3
00:00:11,000 --> 00:00:13,000
Microservices. For the same,

4
00:00:13,000 --> 00:00:18,000
first, let me walk you through the approach that we are going to take, how we are going to implement

5
00:00:18,000 --> 00:00:23,000
it, how the tools like promptaile, Grafana, Loki they are going to interact with each other.

6
00:00:23,000 --> 00:00:28,000
So let me give some official documentation information, post that by taking this as a reference we

7
00:00:28,000 --> 00:00:32,000
can try to implement the same inside our microservices as well.

8
00:00:32,000 --> 00:00:39,000
If you see here, I came to the official documentation of Loki and here I clicked on this getting started.

9
00:00:39,000 --> 00:00:45,000
If you can scroll down on this page, there is a beautiful diagram which explains how to implement log

10
00:00:45,000 --> 00:00:47,000
aggregation with the help of Grafana and its tools.

11
00:00:47,000 --> 00:00:51,000
If you see here first, there is a application with the name Flag.

12
00:00:51,000 --> 00:00:55,000
This is just a random application provided by the Grafana team.

13
00:00:55,000 --> 00:01:00,000
This application is going to generate and emit logs continuously.

14
00:01:00,000 --> 00:01:01,000
That's what you can see here.

15
00:01:01,000 --> 00:01:05,000
It is going to emit the logs and this log is deployed into a container.

16
00:01:05,000 --> 00:01:12,000
Now if you see in the same network where my log is deployed, the prompt tell which which is a log agent

17
00:01:12,000 --> 00:01:13,000
will be running.

18
00:01:13,000 --> 00:01:20,000
The responsibility of this prompt is, to read the new logs whenever generated by the log app and collect

19
00:01:20,000 --> 00:01:21,000
the same logs.

20
00:01:21,000 --> 00:01:28,000
And once the prompt collected those logs it has to send to the Loki, but it will not send directly

21
00:01:28,000 --> 00:01:29,000
to the Loki.

22
00:01:29,000 --> 00:01:32,000
In between there is an edge server like a gateway.

23
00:01:32,000 --> 00:01:36,000
With the help of this gateway it is going to send the logs to the Loki.

24
00:01:36,000 --> 00:01:42,000
Inside Loki there are two components like Loki read component and Loki write component.

25
00:01:42,000 --> 00:01:44,000
Why we need so many components.

26
00:01:44,000 --> 00:01:51,000
Because like I said, Loki is capable of storing any amount of logs. To make it scalable and handle any

27
00:01:51,000 --> 00:01:53,000
amounts of log data.

28
00:01:53,000 --> 00:02:00,000
Grafana team built many separate components, so whenever my prompt is trying to send the logs to the

29
00:02:00,000 --> 00:02:07,000
gateway, the gateway will see what is the URL that is invoked by the client. Here the client will

30
00:02:07,000 --> 00:02:12,000
be prompted and in the case where the request is coming from, the promptaile, it will redirect the

31
00:02:12,000 --> 00:02:14,000
request to the Loki right component.

32
00:02:14,000 --> 00:02:20,000
That's why you can see the arrow here is in the dark color without any dashes.

33
00:02:20,000 --> 00:02:26,000
Once my Loki component receives the logs, it is going to store the logs in a component with the name

34
00:02:26,000 --> 00:02:27,000
Minio.

35
00:02:27,000 --> 00:02:29,000
So all these are internal components.

36
00:02:29,000 --> 00:02:32,000
We don't need to worry much about them behind the scenes

37
00:02:32,000 --> 00:02:37,000
grafana is going to take care of all writing, reading with the help of this gateway.

38
00:02:37,000 --> 00:02:40,000
We don't even have to develop all this gateway,

39
00:02:40,000 --> 00:02:41,000
Loki Read Component,

40
00:02:41,000 --> 00:02:42,000
Loki Write Component.

41
00:02:42,000 --> 00:02:47,000
These are readily available tools which we can deploy inside our microservices.

42
00:02:47,000 --> 00:02:52,000
Now you can see the sequence is the log is generated, the promptaile collected, the logs, send it to

43
00:02:52,000 --> 00:02:56,000
Gateway and the gateway forward the same to the Loki write component.

44
00:02:56,000 --> 00:02:58,000
From the Loki write component.

45
00:02:58,000 --> 00:03:01,000
The logs is written into a storage system called Minio.

46
00:03:01,000 --> 00:03:07,000
Now think like the logs are stored inside the centralized location, which is Minio and Loki.

47
00:03:07,000 --> 00:03:12,000
As a next step, one of the developer wants to understand the logs of a particular microservice.

48
00:03:12,000 --> 00:03:18,000
So what he will do, he will go to the grafana and he will try to search the logs whenever developer

49
00:03:18,000 --> 00:03:23,000
is trying to search the logs inside Grafana, the request will go to the gateway and here you can see

50
00:03:23,000 --> 00:03:24,000
the arrow with the dashes.

51
00:03:24,000 --> 00:03:30,000
The gateway will forward the same request to the Loki read component because the developer only trying

52
00:03:30,000 --> 00:03:37,000
to read the logs and this log read component is going to read the logs from the minio and eventually

53
00:03:37,000 --> 00:03:44,000
the same will be sent back to the Grafana where the developer can see the logs in an UI application.

54
00:03:44,000 --> 00:03:50,000
So this is the overall architecture that is recommended by the Grafana to implement log aggregation.

55
00:03:50,000 --> 00:03:52,000
Please don't worry about this

56
00:03:52,000 --> 00:03:53,000
looks very complex.

57
00:03:53,000 --> 00:03:55,000
I cannot implement this.

58
00:03:55,000 --> 00:03:56,000
Please don't worry about that,

59
00:03:56,000 --> 00:04:00,000
it is going to be super, super easy because now we know Docker,

60
00:04:00,000 --> 00:04:03,000
Docker compose with the scripts provided by the Grafana team.

61
00:04:03,000 --> 00:04:05,000
We can easily implement this.

62
00:04:05,000 --> 00:04:10,000
As of now, you can see inside this architecture we have a sample application with the name Flag.

63
00:04:10,000 --> 00:04:15,000
Instead of this flag, we are going to have all our microservices generating the logs and prompt is

64
00:04:15,000 --> 00:04:18,000
going to read them and send to the Loki from the Loki

65
00:04:18,000 --> 00:04:21,000
we are going to read them and see them inside the Grafana.

66
00:04:21,000 --> 00:04:22,000
I hope this is clear.

67
00:04:22,000 --> 00:04:28,000
If you can scroll down, there are steps that we can follow to implement the same inside any microservices

68
00:04:28,000 --> 00:04:29,000
network.

69
00:04:29,000 --> 00:04:34,000
So there are some Yaml configurations given by the Grafana team.

70
00:04:34,000 --> 00:04:39,000
By following the same instructions we can implement them inside our microservice as well.

71
00:04:39,000 --> 00:04:42,000
I'm going to mention this URL inside the GitHub repo.

72
00:04:42,000 --> 00:04:45,000
Please read all the details inside this page.

73
00:04:45,000 --> 00:04:48,000
Whatever I have explained, the same is present inside this page.

74
00:04:48,000 --> 00:04:51,000
I also mentioned all those details inside this slide.

75
00:04:51,000 --> 00:04:53,000
For your quick reference.

76
00:04:53,000 --> 00:05:00,000
If you try to recall the 15 factor methodology that we discussed previously with this approach, we

77
00:05:00,000 --> 00:05:00,000
are

78
00:05:00,000 --> 00:05:06,000
trying to implement one of the recommendations mentioned inside the 15 factor methodology under the

79
00:05:06,000 --> 00:05:07,000
name logs.

80
00:05:07,000 --> 00:05:13,000
So what does 15 factor methodology is recommending to treat the logs as events stream to the standard

81
00:05:13,000 --> 00:05:17,000
output and not concerned with how they are processed or stored.

82
00:05:17,000 --> 00:05:20,000
So the individual microservices are the developers.

83
00:05:20,000 --> 00:05:23,000
They should not worry about streaming or storing the logs.

84
00:05:23,000 --> 00:05:27,000
All the log aggregation should happen from the outside.

85
00:05:27,000 --> 00:05:29,000
That's what we are trying to achieve here.

86
00:05:29,000 --> 00:05:32,000
I hope you are clear with this approach from the next lecture,

87
00:05:32,000 --> 00:05:35,000
let's try to implement the same inside our microservices.

88
00:05:35,000 --> 00:05:38,000
Thank you and I'll catch you in the next lecture bye.

